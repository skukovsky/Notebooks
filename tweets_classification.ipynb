{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2JdzEXmwRU5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtJxkhKpYK2"
      },
      "source": [
        "# Семантическая классификация твитов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXjhtsfF_gBK"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n",
        "!unzip archive.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Y5CHRm6NFe"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import nltk\n",
        "import gensim\n",
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Lb0wbESrgQ"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed(42)\n",
        "torch.cuda.random.manual_seed_all(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Wv-4bu83Fl"
      },
      "source": [
        "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jST2tjgjCTWD"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBwe0wR83C2"
      },
      "source": [
        "examples = data[\"text\"].sample(10)\n",
        "print(\"\\n\".join(examples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8hUK-jnQg6O"
      },
      "source": [
        "indexes = np.arange(data.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "dev_size = math.ceil(data.shape[0] * 0.8)\n",
        "\n",
        "dev_indexes = indexes[:dev_size]\n",
        "test_indexes = indexes[dev_size:]\n",
        "\n",
        "dev_data = data.iloc[dev_indexes]\n",
        "test_data = data.iloc[test_indexes]\n",
        "\n",
        "dev_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ivcpeFoCnZA"
      },
      "source": [
        "## Text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsNHNDES9ZVF"
      },
      "source": [
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n",
        "print(\" \".join(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcBS_u_hTuxp"
      },
      "source": [
        "filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "print(\" \".join(filtered_line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhHNMNYJkNLx"
      },
      "source": [
        "Model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cACJpje2T5bc"
      },
      "source": [
        "word2vec = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafmYHrkT5YD"
      },
      "source": [
        "emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n",
        "print(sum(emb_line).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTS6LCkd6_E7"
      },
      "source": [
        "Ebmeddings normalization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PyLTZ6xf3Oq"
      },
      "source": [
        "mean = np.mean(word2vec.vectors, 0)\n",
        "std = np.std(word2vec.vectors, 0)\n",
        "norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n",
        "print(sum(norm_emb_line).shape)\n",
        "print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4eZajF7pZ1X"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens)\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        line = self.tokenizer.tokenize(text.lower())\n",
        "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "\n",
        "        return filtered_line\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        embeddings = [(word2vec.get_vector(w) - self.mean) / self.std for w in tokens \\\n",
        "                      if w in word2vec and len(w) > 3]\n",
        "\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZJpttbXpZyz"
      },
      "source": [
        "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AhHrWa196Yc"
      },
      "source": [
        "## Average embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScdokSW-994t"
      },
      "source": [
        "indexes = np.arange(len(dev))\n",
        "np.random.shuffle(indexes)\n",
        "example_indexes = indexes[::1000]\n",
        "\n",
        "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
        "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
        "print(len(examples[\"features\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFZRSHdtIac"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)      \n",
        "examples[\"transformed_features\"] = pca.fit_transform(examples['features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szEOWdiNtIX8"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OONK8ldtIWe"
      },
      "source": [
        "draw_vectors(\n",
        "    examples[\"transformed_features\"][:, 0], \n",
        "    examples[\"transformed_features\"][:, 1], \n",
        "    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1XapsADtITv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "num_workers = 4\n",
        "\n",
        "def average_emb(batch):\n",
        "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
        "\n",
        "\n",
        "train_size = math.ceil(len(dev) * 0.8)\n",
        "\n",
        "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U--T2Gjw1r27"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n",
        "    model.train()\n",
        "    for batch in pbar:\n",
        "        features = batch[\"features\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        preds = model(features)\n",
        "        loss = criterion(preds, targets) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}\")\n",
        "    \n",
        "\n",
        "def testing(model, criterion, test_loader, device=\"cpu\"):\n",
        "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
        "    mean_loss = 0\n",
        "    mean_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            features = batch[\"features\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            preds = model(features)\n",
        "            loss = criterion(preds, targets) \n",
        "            acc = torch.sum(torch.argmax(preds, dim=1) == targets).float() / len(targets)\n",
        "\n",
        "            mean_loss += loss.item()\n",
        "            mean_acc += acc.item()\n",
        "\n",
        "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
        "\n",
        "    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
        "\n",
        "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBoZ4F3Fx1Hm"
      },
      "source": [
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "class NLP_model(nn.Module):\n",
        "    def __init__(self, vector_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            nn.Linear(vector_size, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layer_1(x)\n",
        "\n",
        "vector_size = dev.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "model =  NLP_model(vector_size, num_classes)\n",
        "model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss() # Твой лосс\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKhk71Pmx1F1"
      },
      "source": [
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model, optimizer, criterion, train_loader, e, device)\n",
        "    log = testing(model, criterion, valid_loader, device)\n",
        "    print(log)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4dGwD4x1Dt"
      },
      "source": [
        "test_loader = DataLoader(\n",
        "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
        "\n",
        "print(testing(model, criterion, test_loader, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRvzpldHSAu0"
      },
      "source": [
        "## Embeddings for unknown words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxhEpKalU1UQ"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "\n",
        "class TwitterDataset_2(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens)\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "    \n",
        "        line = self.tokenizer.tokenize(text.lower())\n",
        "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) >= 3]\n",
        "\n",
        "        return filtered_line\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "         \n",
        "        embeddings = []\n",
        "        for idx, w in enumerate(tokens):\n",
        "            if w in word2vec and len(w) > 3:\n",
        "                embeddings.append((word2vec.get_vector(w) - self.mean) / self.std)\n",
        "            elif w not in word2vec:\n",
        "                embeddings_left = tokens[0:idx]\n",
        "                embeddings_right = tokens[idx + 1::]\n",
        "                embeddings_sum = embeddings_left + embeddings_right\n",
        "\n",
        "                for another_word in embeddings_sum:\n",
        "                    if another_word in word2vec and len(another_word) > 3:\n",
        "                        embeddings.append((word2vec.get_vector(another_word) - self.mean) / self.std)\n",
        "\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
        "        else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na2iGEVqejyi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTI-BZFpUzze"
      },
      "source": [
        "dev_2 = TwitterDataset_2(dev_data, \"text\", \"emotion\", word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OLmzbbjMwss"
      },
      "source": [
        "train_size_2 = math.ceil(len(dev_2) * 0.8)\n",
        "\n",
        "train_2, valid_2 = random_split(dev_2, [train_size_2, len(dev_2) - train_size_2])\n",
        "\n",
        "train_loader_2 = DataLoader(train_2, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader_2 = DataLoader(valid_2, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBQDUsNqMw5-"
      },
      "source": [
        "vector_size = dev_2.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "model_2 =  NLP_model(vector_size, num_classes)\n",
        "model_2 = model_2.cuda()\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiO4z3qOlJt"
      },
      "source": [
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model_2, optimizer_2, criterion, train_loader_2, e, device)\n",
        "    log = testing(model_2, criterion, valid_loader_2, device)\n",
        "    print(log)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model_2.state_dict(), \"model.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LR8rgSzXPG"
      },
      "source": [
        "test_loader_2 = DataLoader(\n",
        "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model_2.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
        "\n",
        "print(testing(model_2, criterion, test_loader_2, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqNBIG2xDgxa"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=200)\n",
        "qzz = vectorizer.fit_transform(dev_data['text']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrBC-wc4kBPB"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\n",
        "        \n",
        "        self.data = data\n",
        "\n",
        "        self.feature_column = feature_column\n",
        "        self.target_column = target_column\n",
        "\n",
        "        self.word2vec = word2vec\n",
        "\n",
        "        self.label2num = lambda label: 0 if label == 0 else 1\n",
        "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
        "        self.std = np.std(word2vec.vectors, axis=0)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.data[self.feature_column][item]\n",
        "        label = self.label2num(self.data[self.target_column][item])\n",
        "\n",
        "        tokens = self.get_tokens_(text)\n",
        "        embeddings = self.get_embeddings_(tokens)\n",
        "\n",
        "        return {\"feature\": embeddings, \"target\": label}\n",
        "\n",
        "    def get_tokens_(self, text):\n",
        "        line = self.tokenizer.tokenize(text.lower())\n",
        "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "\n",
        "        return filtered_line\n",
        "\n",
        "    def get_embeddings_(self, tokens):\n",
        "        \n",
        "        embeddings = [(word2vec.get_vector(w) - self.mean) / self.std for w in tokens]\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = np.zeros((1, self.word2vec.vector_size)) + qzz\n",
        "        else:\n",
        "            embeddins = np.array(embeddings) + qzz\n",
        "            if len(embeddings.shape) == 1:\n",
        "                embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9W40B3XB5aF"
      },
      "source": [
        "dev_3 = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7bG4AqEECk8"
      },
      "source": [
        "train_size_3 = math.ceil(len(dev_3) * 0.8)\n",
        "\n",
        "train_3, valid_3 = random_split(dev_3, [train_size_3, len(dev_3) - train_size_3])\n",
        "\n",
        "train_loader_3 = DataLoader(train_3, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
        "valid_loader_3 = DataLoader(valid_3, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUlvbdhfE-Jk"
      },
      "source": [
        "vector_size = dev_3.word2vec.vector_size\n",
        "num_classes = 2\n",
        "lr = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "model_3 =  NLP_model(vector_size, num_classes)\n",
        "model_3 = model_3.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_3 = torch.optim.Adam(model_3.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksOv_xqMFE2R"
      },
      "source": [
        "best_metric = np.inf\n",
        "for e in range(num_epochs):\n",
        "    training(model_3, optimizer_3, criterion, train_loader_3, e, device)\n",
        "    log = testing(model_3, criterion, valid_loader_3, device)\n",
        "    print(log)\n",
        "    if log[\"Test Loss\"] < best_metric:\n",
        "        torch.save(model_3.state_dict(), \"model.pt\")\n",
        "        best_metric = log[\"Test Loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1-1HymLFKLm"
      },
      "source": [
        "test_loader_3 = DataLoader(\n",
        "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False, \n",
        "    collate_fn=average_emb)\n",
        "\n",
        "model_3.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
        "\n",
        "print(testing(model_3, criterion, test_loader_3, device=device))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}